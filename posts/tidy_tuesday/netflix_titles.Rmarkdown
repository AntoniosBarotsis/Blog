---
author: "Antonios Barotsis"
title: "Netflix Titles"
date: 2021-08-01T20:10:48+02:00
description: "Exploring the \"Netflix Movies and TV Shows\" Kaggle dataset"
tags: ["R", "Data", "Plot", "Machine Learning"]
categories: ["TidyTuesday"]
ShowToc: false
---



```{r setup, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  results = TRUE,
  out.width = "100%",
  echo = FALSE
)
```

## Load the weekly Data

We load the `tidytuesdayR` along with `tidyverse` and `ggthemr` for plot themes.

```{r load, echo = TRUE, results = FALSE}
library(tidytuesdayR)
library(tidyverse)
library(tidymodels)
library(ggthemr)
library(ggridges)
library(lubridate)
library(plotly)
library(ranger)
library(kernlab)

tuesdata <- tidytuesdayR::tt_load('2021-04-20')
netflix <- tuesdata$netflix

ggthemr('flat dark', type = 'outer')
```



## Readme

The link to the original readme file can be found [here](https://github.com/rfordatascience/tidytuesday/blob/master/data/2021/2021-04-20/readme.md).

This data set contains Movies and TV Shows available on Netflix with information
release date, rating, duration, genre and more. I want to mainly explore how
both Movies and TV Shows changed over time in relation to different parameters.


## Glimpse Data

We can start by getting an idea of the data

```{r, echo = TRUE}
# Check the `type` variable
netflix %>%
  count(type)

# See the amount of countries
netflix %>%
  count(country) %>%
  nrow()

# Check the date range
netflix %>%
  summarise(min(release_year), max(release_year))
```


We can also make a few explanatory plots to get a better idea of the data.

```{r}
ggplotly(
  ggplot(netflix, aes(release_year, fill = type)) + 
    geom_histogram(binwidth = 1) +
    xlab("Release Year") +
    ylab("Frequency")
)
```  
  
<br />

From the plot above we can see that although movies greatly outnumber TV Shows
as of now, they have drastically slowed down their increase and even started
decreasing by 2020 unlike TV Shows that are still growing exponentially in the
last 5 years. It is worth noting that this dataset was mined before the end of
2020 which means that the values from that year may be underrepresented. Despite
that however, the above observation still holds as we can very clearly see a
steady decline in the addition of new movies that begins at 2018.

<br />

```{r}
ggplotly(
  ggplot(netflix, aes(release_year, fill = type)) + 
    geom_histogram(binwidth = 2.5) +
    facet_wrap(~ type, ncol = 2, scale = "free_y") +
    xlab("Release Year") +
    ylab("Frequency") +
    theme(axis.title = element_blank(),strip.text.x = element_blank())
)
```

<br />

Here we can see another comparison of Movies and TV Shows side by side, binned
at 5 years.

<br />

```{r}
ggplotly(
  netflix %>%
    count(five_years = 5 * (release_year %/% 5), type) %>%
    group_by(type) %>%
    mutate(percent = n / sum(n)) %>%
    ggplot(aes(five_years, percent, color = type)) +
    geom_line() +
    xlab("Release Year") +
    ylab("Percentage")
)
```
<br />

Here we can see the rate of change more clearly. 

## Wrangle

I noticed that the `duration` was a text field instead of a numerical one so I
decided to split that to 2 fields; `duration` and `duration_unit` so as to be
able to make some plots on it. I also converted the `date_added` variable
from a string to the year it's referring to for the same reason. I also created
a new variable `duration_hours` which is `duration` divided by 60, this only
makes sense for Movies of course. Lastly I dropped any entries with Null columns.

<br />

```{r}
netflix_wrangled <- netflix %>%
  drop_na() %>%
  separate(
      duration,
      c("duration", "duration_unit"),
      sep = " ", convert = TRUE
  ) %>%
  separate_rows(listed_in, sep = "(,|\\s&) ") %>%
  mutate(
    date_added = mdy(date_added),
    duration_hours = round(duration / 60)
  )

tmp <- netflix_wrangled %>%
  filter(type == "Movie") %>%
  mutate(
    five_years = 5 * (release_year %/% 5)
  )
```

Let's make some more plots!

### Movie Duration over the Years

```{r}
tmp %>%
  filter(duration_hours < 5) %>% #only 4
  select(five_years, duration_hours) %>%
  ggplot(aes(x = five_years,y = duration_hours, group = duration_hours)) +
  geom_density_ridges(bandwidth = 5, rel_min_height = 0.01) + 
  xlim(c(1940,2020)) +
  xlab("Release Year") +
  ylab("Duration in Hours (rounded)")
```

<br />

I think this showcases the shift in the duration of movies over the years very
nicely; we can see that movies that are under an hour only got released almost
exclusively after the year 2000. The more "standard" movie duration of 2-3 hours
is similar but we can see that it started picking up traction a bit before 1-hour
movies. Lastly we can see that really long movies (3 hours or 4 especially) were
a lot more common before the 2000s. If we also account for the fact that more
movies were produced after the 2000s (from this data set) we could say that
long movies have been in decline in recent years.

Another great way to show the decline in duration is this plot which tells the
same story and also shows the few outliars from recent years.

```{r}
ggplotly(
  tmp %>%
    ggplot(aes(x = five_years,y = duration, group = five_years)) +
    geom_boxplot() +
    xlab("Release Year") +
    ylab("Duration (Minutes)")
)
```

<br />

We can actually test this hypothesis with a T Test.

```{r echo = TRUE, message = TRUE}
ttest_data <- netflix_wrangled %>% 
  filter(type == "Movie") %>% 
  mutate(is_short = factor(ifelse(duration_hours < 3, TRUE, FALSE))) %>%
  select(release_year, is_short)

t.test(
    release_year ~ is_short,
    data = ttest_data,
    conf.level = 0.95
)
```


We see that there is a statistically significant difference between the average
release year for movies that have a 3+ hour duration and shorter duration movies.
Take this with a grain of salt however as the population difference between the
short and long movies is very big (600 vs 11000). I will revisit this at the end
of this post and try to model it using a few machine learning algorithms.

### Media Categories (Genres)

```{r}
ggplotly(
  netflix_wrangled %>% 
    group_by(listed_in) %>% 
    summarise(n = median(duration)) %>% 
    arrange(desc(n)) %>%
    head(10) %>%
    ggplot(aes(reorder(listed_in, n), n)) + geom_col() + coord_flip() +
    xlab("Category") +
    ylab("Median Duration")
)
```

<br />

We can also check the median duration of each category.

```{r}
ggplotly(
  netflix_wrangled %>%
    transmute(listed_in, date_added = year(date_added), release_year) %>%
    filter(date_added != release_year) %>%
    group_by(listed_in) %>%
    summarise(median_years_to_be_added = median(date_added - release_year)) %>%
    arrange(desc(median_years_to_be_added)) %>% {
      rbind(
        head(., 5) %>% mutate(portion = "Highest"), 
        tail(., 5) %>% mutate(portion = "Lowest")
        )
    } %>%
    ggplot(aes(
      reorder(listed_in, median_years_to_be_added), 
      median_years_to_be_added, 
      fill = portion)
    ) +
    geom_col() +
    coord_flip() +
    xlab("Category") +
    ylab("Median Amount of Years to be Added")
)
```

<br />

Another potentially interesting metric we could explore is the median amount of
years passed between the production and upload to netflix for both Movies and
TV Shows. In the graph I took the 5 longest and 5 shortest pieces of media
when it comes to that "upload time". We can see that in TV Shows take up all the
5 spots for lowest upload times. That could be attributed to a general drop in
upload time and that along with the fact that TV shows started appearing mostly
recently could explain this.

<br />

I want to shift the interest over to the ratings for a bit which I also found
interesting.

```{r}
ggplotly(
  netflix_wrangled %>%
    count(listed_in, sort = TRUE) %>%
    mutate(listed_in = factor(listed_in)) %>%
    head(5) %>%
    ggplot(aes(reorder(listed_in, n), n)) + geom_col() +
    xlab("Category") +
    ylab("Amount")
)
```

<br />

Here we see that most pieces of media from this data set are included in the
"International Movies", "Dramas" and "Comedies" categories.


### Ratings

We can also take a look at the ratings these Movies and Shows have.

```{r}
ggplotly(
  netflix_wrangled %>%
    filter(release_year >= 2010) %>%
    drop_na(rating) %>%
    count(release_year, rating = fct_lump(rating, 5)) %>%
    ggplot(aes(release_year, n, fill = rating)) + geom_area() +
    xlab("Release Year") +
    ylab("Frequency")
)
```
<br />

I was actually fairly surprised to see that adult ratings are that popular.

```{r}
ggplotly(
  area2 <- netflix_wrangled %>%
    filter(release_year >= 2010) %>%
    drop_na(rating) %>%
    count(release_year, type, rating = fct_lump(rating, 5)) %>%
    ggplot(aes(release_year, n, fill = rating)) + 
    geom_bar(stat="identity", width = 1) +
    xlab("Release Year") +
    ylab("Frequency")
)
```
<br />

We can see the same information plotted differently here.


## Modelling

After some trial and error, I decided to try and model the `duration` using
`release_year` and `listed_in`. I changed `release_year` to start from
the smallest year in the dataset and I applied one hot encoding to `listed_in`.
Since there are way less movies under 3 hours, I want my model
to be able to pick up almost all of them; I want to keep false negatives
as low as possible so as to make the model more sensitive. The model yields
92% accuracy with only 2 false negatives! Seems like our intuition about
the release year along with the categories a movie is listed in is a very decent
predictor of its duration. I tried modeling with Logistic Regression,
K Nearest Neighbors and Random Forest. I used 10 fold cross validation and
hyper parameter tuning in the last two to increase the performance. I found
random forests to be the most accurate. I decided to hide the code for this
as it is quite lengthy but if you are interested you can always check the source
file [here]().


```{r}
set.seed(123456)

# `is_short` is defined as a movie with a length of less than 3 hours
# `release_year` is adjusted to now start from the smallest year in the dataset
# Lastly, one hot encoding is applied and an 80-20 split to a training and testing
# dataset is made
model_data <- netflix_wrangled %>%
  filter(type == "Movie") %>% 
  mutate(
    is_short = factor(ifelse(duration_hours < 3, TRUE, FALSE)),
    release_year = release_year - min(release_year),
  ) %>%
  select(is_short, release_year, listed_in) %>%
  pivot_wider(
    names_from = listed_in, 
    values_from = listed_in, 
    values_fn = list(listed_in = ~1),
    values_fill = list(listed_in = 0)
  ) %>%
  initial_split(prop = 0.8)

# Get training and testing sets
model_train <- training(model_data)
model_test <- testing(model_data)

# Create folds
folds <- vfold_cv(model_train, v = 10)

# Create model with parameters obtained from tuning
model <- rand_forest(
  mode = "classification",
  engine = "ranger",
  mtry = 4,
  trees = 28,
  min_n = 40
)

# Fit
model_fit <- model %>%
  fit(is_short ~ ., data = model_train)

# Create predictions
model_predictions <- model_fit %>%
  predict(model_test) %>%
  mutate(truth = model_test$is_short)

# Get confusion matrix and the accuracy
c_matrix <- table(model_predictions)
model_accu <- accuracy(c_matrix)

# # Models obtained from tuning
# 
# # Random Forest
# model <- rand_forest(
#   mode = "classification",
#   engine = "ranger",
#   mtry = 4,
#   trees = 28,
#   min_n = 40
# )
# 
# # K-NN
# model <- nearest_neighbor(
#   mode = "classification",
#   engine = "kknn",
#   neighbors = 12,
#   weight_func = "biweight",
#   dist_power = 0.734
# )
# 
# # Logistic Regression
# model <- logistic_reg(
#   mode = "classification",
#   engine = "glm"
# )
# 
# 
# # Hyper parameter tuning
# 
# # Create recipe, normalize and downsample the data to account in asymetry
# model_data_preproc <- recipe(
#     is_short ~ .,
#     data = model_train
#   ) %>%
#   step_normalize(all_predictors()) %>%
#   step_downsample(is_short)
# 
# # Create Random Forest model
# rf_mod <- rand_forest(
#   mode = "classification",
#   engine = "ranger",
#   mtry = tune(),
#   trees = tune(),
#   min_n = tune()
# )
# 
# # Create K Nearest Neighbors model
# knn_mod <- nearest_neighbor(
#   mode = "classification",
#   engine = "kknn",
#   neighbors = tune(),
#   weight_func = tune(),
#   dist_power = tune()
# )
# 
# # Create a workflow for each model
# rf_workflow <- workflow() %>%
#   add_model(rf_mod) %>%
#   add_recipe(model_data_preproc)
# 
# knn_workflow <- workflow() %>%
#   add_model(knn_mod) %>%
#   add_recipe(model_data_preproc)
# 
# # Specify the mtry param
# rf_set <- parameters(rf_workflow) %>%
#   update(mtry = mtry((c(1L,10L))))
# 
# knn_set <- parameters(knn_workflow)
# 
# # Set up doParallel backend for cross validation
# all_cores <- parallel::detectCores(logical = FALSE)
# 
# library(doParallel)
# cl <- makePSOCKcluster(all_cores)
# registerDoParallel(cl)
# 
# 
# set.seed(123456)
# 
# # Random Forest
# search_res_rf <-
#   rf_workflow %>%
#   tune_bayes(
#     resamples = folds,
#     param_info = rf_set,
#     initial = 5,
#     iter = 50,
#     metrics = metric_set(pr_auc),
#     control = control_bayes(no_improve = 30, verbose = TRUE)
#   )
# 
# # K Nearest Neighbors
# search_res_knn <-
#   knn_workflow %>%
#   tune_bayes(
#     resamples = folds,
#     param_info = knn_set,
#     initial = 5,
#     iter = 10,
#     metrics = metric_set(pr_auc),
#     control = control_bayes(no_improve = 30, verbose = TRUE)
#   )
# 
# # Find best hyper parameters and inspect them
# select_best(search_res_rf, metric = "pr_auc")
# select_best(search_res_knn, metric = "pr_auc")
```

